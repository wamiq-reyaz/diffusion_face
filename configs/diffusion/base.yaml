seq_length: ${diffusion_length_resolver:${dataset.base_seq_len},${dataset.padding},${conditioner.seq_length},${model.condition_is_attention}}
timesteps: 1000
sampling_timesteps: 100
objective: 'pred_v'
beta_schedule: 'cosine'
beta_kwargs:
  beta_start: 0.0001
  beta_end: 0.02
  beta_s: 0.008
ddim_sampling_eta: 0.0
auto_normalize: True
is_self_denoising: False
loss_type: 'l1'
loss_kwargs: {}
return_latents: False